model:
  target: minllm.models.gpt.GPT
  params:
    context_length: 512
    # Spacy tokenizer size of 40478 + 3 special tokens, padded up to nearest multiple of 64 for efficiency
    vocab_size: 40512
    num_layers: 12
    num_attention_heads: 12
    embedding_dim: 768
    dropout: 0.1
    # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
    bias: True

tokenizer:
  target: minllm.tokenizer.bpe.spacy.SpacyTextTokenizer
  params:
    special_tokens: ['_start_', '_delimiter_', '_classify_']

# The BooksCorpus dataset has 1033957426 examples
# (for a total of 1033957938 tokens), which means there are
# about ~1b tokens per epoch. GPT-1 was trained
# for 100 epochs on a minibatch size of 64 on a 8xA100, which
# means they trained on a total of ~100b tokens of data. Each epoch
# was therefore approximately 32k (1B tokens // context_length = 2m examples // batch_size = 32k batches)
# steps at batch size of 64 with a context length
# of 512. At a batch size of 64, with a context length of 512, there are about
# 32k tokens per step (512 tokens * 64 samples in a batch) on a single node. For our case, we have access
# to a 1xA100 instance, so we will train with a batch size
# of 64 and use gradient accumulation with 5 steps
# to approximate a batch size of 320. GPT-1 was trained for 100 epochs, at 1b tokens
# per epoch, so we want to reproduce 100b tokens of total training. With
# batch_size*context_length*gradient_accumulation_steps = 64*512*5 = 163840 tokens
# per step, we need 100b / 163840 = 610,351 ~ 600k steps of training.
training:
  # Batch size to use for training
  batch_size: 64
  # The number of steps to perform gradient accumulation
  gradient_accumulation_steps: 5
  # Total number of steps to train for, equivalent to 10 epochs
  # given the above batch size.
  training_steps: 600000
  # Weight decay factor for all of the non-bias and layer norm terms
  weight_decay: 0.01
  # True to decay the learning rate using a cosine annealing strategy
  learning_rate_decay: True
  # Number of steps for linear learning rate warmup
  warmup_steps: 2000
  # The maximum step where we cap the learning rate decay
  learning_rate_decay_steps: 600000
  # The base learning rate, without warmup or decay
  learning_rate: .00025
  # The minimum learning rate, if we are using decay
  min_learning_rate: .0000
  # Save and sample the model every N steps
  save_and_sample_every_n: 50000
  # The dataset we are training on
  dataset: "bookscorpus"
  # Mixed precision training settings
  mixed_precision: "bf16"

  # Metrics to evaluate on
  evaluation:
    # The metrics to evaluate on
    metrics: ['perplexity']
    # The dataset to evaluate on. In this case, BooksCorpus
    # does not have a hold out set, so we will evaluate on random
    # samples from the set itself.
    dataset: "bookscorpus"
    # Evalate over a total of N samples
    total_samples: 1024
    # Evaluate 12 samples at a time
    samples_per_batch: 64
    # Context length of the model
    context_length: 512

  # The optimizer to use for training.
  optimizer:
    target: torch.optim.Adam
    params:
      betas: [0.9, 0.95]

