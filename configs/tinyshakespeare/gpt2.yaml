model:
  target: minllm.models.gpt2.GPT
  params:
    context_length: 64
    # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    vocab_size: 50304
    num_layers: 12
    num_attention_heads: 12
    embedding_dim: 768
    dropout: 0.0
    # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
    bias: True

training:
  learning_rate: .0006
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
